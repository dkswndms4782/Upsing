{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39904,
     "status": "ok",
     "timestamp": 1679063327922,
     "user": {
      "displayName": "DK D",
      "userId": "10687755219747411266"
     },
     "user_tz": -420
    },
    "id": "mPhjvjd8nqR5",
    "outputId": "bdfaecf1-591f-4cd6-a899-0510d3c7d007"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6848\\75567182.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "from pandas import DataFrame, Series\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import itertools\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input \n",
    "from tensorflow.keras.layers import Dense, Flatten, MaxPooling2D\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQYlYdsoxeLU"
   },
   "source": [
    "# 1) Make CSV For Spliting Train, Test Data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7LaCf7EoCme"
   },
   "outputs": [],
   "source": [
    "techniques = ['belt','vibrato','vocal_fry', 'straight'] # ['breathy','lip_trill','fast_forte','fast_piano', 'forte','inhaled',  'messa','pp','slow_forte','slow_piano','spoken','trill']\n",
    "train_singers = ['f1','f3','f4','f5','f6','f7','f9',\"m1\",\"m2\",\"m4\",\"m6\",\"m7\",\"m8\",\"m9\",\"m11\"]\n",
    "test_singers = ['f2','f8',\"m3\",\"m5\",\"m10\"]\n",
    "\n",
    "train_df = pd.DataFrame(columns = {'path','singer','technique', 'length'})\n",
    "test_df = pd.DataFrame(columns = {'path','singer','technique', 'length'})\n",
    "\n",
    "path = \"/content/drive/MyDrive/Startup_Hackathon_Jarvis/data_by_technique/\"\n",
    "\n",
    "for technique in techniques:\n",
    "\n",
    "  file_list =  glob.glob(path + technique + \"/*.wav\")\n",
    "\n",
    "  for i in file_list:\n",
    "    singer = re.findall(r'[mf][0-9]', i)\n",
    "    sig, sr = torchaudio.load(i)\n",
    "    length = sig.shape[1]/sr\n",
    "\n",
    "    if not len(singer): continue\n",
    "\n",
    "    if singer[0] in test_singers:\n",
    "      test_df = test_df.append({'path': i,'singer': singer[0],'technique': technique,'length' : length}, ignore_index = True)\n",
    "    \n",
    "    elif singer[0] in train_singers:\n",
    "      train_df = train_df.append({'path': i,'singer': singer[0],'technique': technique,'length' : length}, ignore_index = True)\n",
    "\n",
    "# train_df.to_csv(\"/content/drive/MyDrive/Startup_Hackathon_Jarvis/train_df.csv\")\n",
    "# test_df.to_csv(\"/content/drive/MyDrive/Startup_Hackathon_Jarvis/test_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RJPC4iFoDQ9"
   },
   "source": [
    "# 2) Saving File & read File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCv40rEtFV65"
   },
   "outputs": [],
   "source": [
    "def read_df(name):\n",
    "  return pd.read_csv(f\"/content/drive/MyDrive/Startup_Hackathon_Jarvis/{name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgIgLOAloDWn"
   },
   "outputs": [],
   "source": [
    "def save_csv(df, name, folder_name = \"pkl_data\"):\n",
    "  tmpdf = pd.DataFrame(df)\n",
    "  tmpdf.to_csv(f\"/content/drive/MyDrive/Startup_Hackathon_Jarvis/{folder_name}/{name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgjOxHfFoDaH"
   },
   "outputs": [],
   "source": [
    "def read_pickle(name, folder_name = \"pkl_data\"):\n",
    "  return pd.read_pickle(f\"/content/drive/MyDrive/Startup_Hackathon_Jarvis/{folder_name}/{name}.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2jllitaoDdG"
   },
   "source": [
    "# 3) Show Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQU3kL91Do4F"
   },
   "source": [
    "### # waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sy5wjXzUoDf9"
   },
   "outputs": [],
   "source": [
    "def show_wave(name, data):\n",
    "  plt.figure(figsize=(16, 2))\n",
    "  plt.title(name)\n",
    "  plt.plot(data)\n",
    "  plt.ylim((-0.1, 0.1))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "He9PCk2YoDla"
   },
   "source": [
    "\n",
    "\n",
    "### # spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IolaioU7oDn_"
   },
   "outputs": [],
   "source": [
    "def show_spec(data, name, y_axis, format, x_axis = 'time'):\n",
    "  librosa.display.specshow(data, sr = 44100, x_axis = x_axis, y_axis = y_axis)\n",
    "  plt.colorbar(format = format)\n",
    "  plt.title(name)\n",
    "  plt.tight_layout()\n",
    "  plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0mWkusooDq1"
   },
   "source": [
    "\n",
    "# 4) Data Augmentaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ObJgpSsoDtu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def make_data(df, name, sec = 0.4, save = True):\n",
    "  with tf.device('/GPU:0'):\n",
    "    audio = []\n",
    "    label = []\n",
    "    speed = [1.0, 1.5, 2.0]\n",
    "    for i in tqdm(range(len(df))) :\n",
    "      x, sr = librosa.load(os.path.join('.', df[\"path\"].iloc[i]), sr = 44100)\n",
    "      if df['technique'].iloc[i] not in ['vibrato','straight']:\n",
    "        j = 2\n",
    "        while (j+1)*sec < (df['length'].iloc[i]):\n",
    "          audio.append(x[int(j*sec*44100) : int((j+1)*sec*44100)]) # 0.4초씩 nonoverlapping하게 자르기\n",
    "          label.append(df[\"technique\"].iloc[i])\n",
    "          j += 1\n",
    "\n",
    "      elif df['technique'].iloc[i] == \"straight\":\n",
    "        j = 3\n",
    "        while (j+1)*sec < (df['length'].iloc[i]-2):\n",
    "          if j%2 == 0:\n",
    "            j+=1\n",
    "            continue\n",
    "          audio.append(x[int(j*sec*44100) : int((j+1)*sec*44100)])\n",
    "          label.append(df[\"technique\"].iloc[i])\n",
    "          j += 1\n",
    "        \n",
    "      elif df['technique'].iloc[i] == \"vibrato\":\n",
    "        tmp = librosa.effects.time_stretch(x, rate = speed[i%3])\n",
    "        print(tmp)\n",
    "        k = 3\n",
    "        while (k+1)*sec < (len(tmp)/44100-2):\n",
    "          audio.append(tmp[int(k*sec*44100): int((k+1)*sec*44100)]) # 0.4초씩 nonoverlapping하게 자르기\n",
    "          label.append(df[\"technique\"].iloc[i])\n",
    "          k += 1\n",
    "\n",
    "\n",
    "    if save:\n",
    "      save_csv({\"audio\":audio, 'label':label}, f\"{name}_data\")\n",
    "\n",
    "    return (audio, label)\n",
    "\n",
    "train_df = pd.read_csv(\"/content/drive/MyDrive/Startup_Hackathon_Jarvis/train_df.csv\")\n",
    "test_df = pd.read_csv(\"/content/drive/MyDrive/Startup_Hackathon_Jarvis/test_df.csv\")\n",
    "train_audio, train_label = make_data(train_df, name = \"train\")\n",
    "test_audio, test_label = make_data(test_df, name = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvuhfZ1z71ya"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/content/drive/MyDrive/Startup_Hackathon_Jarvis/train_df.csv\")\n",
    "audio = []\n",
    "label = []\n",
    "speed = [1.0, 1.5, 2.0]\n",
    "sec = 0.4\n",
    "save = True\n",
    "name = \"train\"\n",
    "folder_name = \"pkl_data\"\n",
    "columns = ['audio', 'label']\n",
    "for i in tqdm(range(len(df))) :\n",
    "  x, sr = librosa.load(os.path.join('.', df[\"path\"].iloc[i]), sr = 44100)\n",
    "  if df['technique'].iloc[i] not in ['vibrato','straight']:\n",
    "    j = 2\n",
    "    while (j+1)*sec < (df['length'].iloc[i]):\n",
    "      audio.append(x[int(j*sec*44100) : int((j+1)*sec*44100)]) # 0.4초씩 nonoverlapping하게 자르기\n",
    "      label.append(df[\"technique\"].iloc[i])\n",
    "      j += 1\n",
    "\n",
    "  elif df['technique'].iloc[i] == \"straight\":\n",
    "    j = 3\n",
    "    while (j+1)*sec < (df['length'].iloc[i]-2):\n",
    "      if j%2 == 0:\n",
    "        j+=1\n",
    "        continue\n",
    "      audio.append(x[int(j*sec*44100) : int((j+1)*sec*44100)])\n",
    "      label.append(df[\"technique\"].iloc[i])\n",
    "      j += 1\n",
    "    \n",
    "  elif df['technique'].iloc[i] == \"vibrato\": # ??\n",
    "    tmp = librosa.effects.time_stretch(x, rate = speed[i%3])\n",
    "    k = 3\n",
    "    while (k+1)*sec < ((len(tmp)/44100)-2):\n",
    "      audio.append(tmp[int(k*sec*44100): int((k+1)*sec*44100)]) # 0.4초씩 nonoverlapping하게 자르기\n",
    "      label.append(df[\"technique\"].iloc[i])\n",
    "      k += 1\n",
    "\n",
    "tmpdf = pd.DataFrame({\"audio\" : audio, \"label\" : label})\n",
    "tmpdf.to_csv(f\"/content/drive/MyDrive/Startup_Hackathon_Jarvis/pkl_data/{name}.csv\", columns = ['audio', 'label'], mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQjn0CmUoDwz"
   },
   "outputs": [],
   "source": [
    "def augmentation(audio, label, name, save = True):\n",
    "  with tf.device('/GPU:0'):\n",
    "    audio_pitch_shift = []\n",
    "    for i in tqdm(range(len(audio))):\n",
    "      audio_pitch_shift.append(librosa.effects.pitch_shift(audio[i], 44100, 4))\n",
    "\n",
    "    noise = np.random.randn(len(audio[0]))\n",
    "    noise = (noise - min(noise)) / (max(noise) - min(noise))\n",
    "    audio_noise = audio + 0.1 * noise\n",
    "    # Cast back to same data type\n",
    "    audio_noise = audio_noise.astype(type(audio[0]))\n",
    "\n",
    "\n",
    "    audio = np.concatenate([audio, audio_pitch_shift, audio_noise],  axis=0) \n",
    "    label = np.concatenate([label, label, label]) \n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    audio = scaler.fit_transform(audio)\n",
    "\n",
    "    tmp = []\n",
    "    for i in tqdm(range(len(audio))):\n",
    "      tmp.append([audio[i], label[i]])\n",
    "\n",
    "    if save:\n",
    "      save_pickle(tmp, f\"augmented_{name}_data\")\n",
    "\n",
    "    return (audio, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSz73uA1oDz2"
   },
   "outputs": [],
   "source": [
    "def make_mel(audio, label, name, save = True):\n",
    "    with tf.device('/GPU:0'):\n",
    "      audio_melspectrogram = []\n",
    "      for x in audio :\n",
    "        ret = librosa.feature.melspectrogram(y = x, sr = 44100)\n",
    "        audio_melspectrogram.append(ret)\n",
    "\n",
    "      amp_db = []\n",
    "      for i in range(len(audio_melspectrogram)) :\n",
    "        amp = librosa.amplitude_to_db(np.abs(audio_melspectrogram[i])) ## np.abs?\n",
    "        amp_db.append(amp)\n",
    "\n",
    "      features = []\n",
    "      for i in range(len(amp_db)) :\n",
    "        features.append([amp_db[i], label[i]])\n",
    "    \n",
    "      if save:\n",
    "        save_pickle(features, f\"mel_{name}_data\")\n",
    "\n",
    "      return pd.DataFrame(features, columns = ['audio', 'label'])\n",
    "train_mel = make_mel(train_audio, train_label, name = \"nonaugmented_train\", save = True)\n",
    "test_mel = make_mel(test_audio, test_label, name = \"nonaugmented_test\", save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPHi83nkoD51"
   },
   "outputs": [],
   "source": [
    "def make_mfcc(audio, label, name = \"\", save = True):\n",
    "  with tf.device('/GPU:0'):\n",
    "      audio_mfcc = []\n",
    "      for x in audio:\n",
    "        ret = librosa.feature.mfcc(y=x, sr=44100, n_mfcc=20)\n",
    "        audio_mfcc.append(ret)\n",
    "\n",
    "      amp_db = []\n",
    "      for i in range(len(audio_mfcc)) :\n",
    "        amp = librosa.amplitude_to_db(np.abs(audio_mfcc[i])) \n",
    "        amp_db.append(amp)\n",
    "\n",
    "      features = []\n",
    "      for i in range(len(amp_db)) :\n",
    "        features.append([amp_db[i], label[i]])\n",
    "    \n",
    "      if save:\n",
    "        save_pickle(features, f\"mfcc_{name}_data\")\n",
    "\n",
    "      return pd.DataFrame(features, columns = ['audio', 'label'])\n",
    "\n",
    "train_mfcc = make_mfcc(train_audio, train_label, name = \"nonaugmented_train\", save = True)\n",
    "test_mfcc = make_mfcc(test_audio, test_label, name = \"nonaugmented_test\", save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGcHISXuoD_6"
   },
   "outputs": [],
   "source": [
    "# https://hyongdoc.tistory.com/401\n",
    "# n_fft가 win_length보다 클 경우, 큰 구간은 모두 zero padding으로 채우게 됩니다. 이유는 설명드린바와 같이, 주파수 해상도를 높이기 위해서입니다.\n",
    "def make_stft(audio, label, name, save = True):\n",
    "    with tf.device('/GPU:0'):\n",
    "      audio_stft = []\n",
    "      for x in audio:\n",
    "        ret = librosa.stft(x, n_fft=2048, win_length = 2048, hop_length=512)\n",
    "        audio_stft.append(ret)\n",
    "\n",
    "      amp_db = []\n",
    "      for i in range(len(audio_stft)) :\n",
    "        amp = librosa.amplitude_to_db(np.abs(audio_stft[i]))\n",
    "        amp_db.append(amp)\n",
    "\n",
    "      features = []\n",
    "      for i in range(len(amp_db)) :\n",
    "        features.append([amp_db[i], label[i]])\n",
    "\n",
    "      if save:\n",
    "         save_pickle(features, f\"stft_{name}_data\")\n",
    "\n",
    "      return pd.DataFrame(features, columns = ['audio', 'label'])\n",
    "\n",
    "train_stft = make_stft(train_audio, train_label, name = \"nonaugmented_train\", save = True)\n",
    "test_stft = make_stft(test_audio, test_label, name = \"nonaugmented_test\", save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qglbTfZToEC9"
   },
   "outputs": [],
   "source": [
    "# (2048, 512, 128)\n",
    "# Investigating Time-Frequency Representations for Audio Feature Extraction in Singing Technique Classification\n",
    "# 이 논문에서 최고성능을 낸 데이터타입이라 구현해봄\n",
    "def make_multi_resolution_spectrogram(audio, label, name, save = True):\n",
    "  with tf.device('/GPU:0'):\n",
    "      audio_multi = []\n",
    "      for x in audio:\n",
    "        stft1 = librosa.stft(x, n_fft=2048, win_length = 2048, hop_length=512)\n",
    "        stft2 = librosa.stft(x, n_fft=2048, win_length = 512, hop_length=512)\n",
    "        stft3 = librosa.stft(x, n_fft=2048, win_length = 128, hop_length=512)\n",
    "        audio_multi.append([stft1, stft2, stft3])\n",
    "\n",
    "      amp_db = []\n",
    "      for i in range(len(audio_multi)) :\n",
    "        amp1 = librosa.amplitude_to_db(np.abs(audio_multi[i][0])) \n",
    "        amp2 = librosa.amplitude_to_db(np.abs(audio_multi[i][1]))\n",
    "        amp3 = librosa.amplitude_to_db(np.abs(audio_multi[i][2]))\n",
    "        amp_db.append([amp1, amp2, amp3])\n",
    "\n",
    "      features = []\n",
    "      for i in range(len(amp_db)) :\n",
    "        features.append([amp_db[i], label[i]])\n",
    "\n",
    "      if save:\n",
    "         save_pickle(features, f\"multi_{name}_data\")\n",
    "\n",
    "      return pd.DataFrame(features, columns = ['audio', 'label'])\n",
    "train_multi = make_multi_resolution_spectrogram(train_audio, train_label, name = \"nonaugmented_train\", save = True)\n",
    "test_multi = make_multi_resolution_spectrogram(test_audio, test_label, name = \"nonaugmented_test\", save = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prLDw40CoEGL"
   },
   "source": [
    "\n",
    "# 5) Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgVhVRVxoEMQ"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "def train_test(train_features, test_features, shuffle = True, wave_flag=False, labelEncoder = True):\n",
    "  x_train = np.array(train_features.audio.tolist())\n",
    "  y_train = np.array(train_features.label.tolist())\n",
    "  x_test = np.array(test_features.audio.tolist())\n",
    "  y_test = np.array(test_features.label.tolist())\n",
    "\n",
    "  if labelEncoder:\n",
    "    le = LabelEncoder()\n",
    "    y_train = to_categorical(le.fit_transform(y_train))\n",
    "    y_test = to_categorical(le.transform(y_test))\n",
    "    print(le.classes_)\n",
    "\n",
    "  \n",
    "  if shuffle: \n",
    "    tmp = list(zip(x_train, y_train))\n",
    "    random.shuffle(tmp)\n",
    "    x_train, y_train = zip(*tmp)\n",
    "\n",
    "    tmp1 = list(zip(x_test, y_test))\n",
    "    random.shuffle(tmp1)\n",
    "    x_test, y_test = zip(*tmp1)\n",
    "    x_train, y_train, x_test, y_test = np.array(x_train,dtype=np.float32), np.array(y_train,dtype=np.float32), np.array(x_test,dtype=np.float32), np.array(y_test,dtype=np.float32)\n",
    "  \n",
    "  if wave_flag:\n",
    "    print(x_train.shape)\n",
    "    x_train = tf.reshape(x_train.astype(float), [-1, x_train.shape[1], 1]) ### 확인\n",
    "    x_test = tf.reshape(x_test.astype(float), [-1,  x_train.shape[1], 1])\n",
    "    return x_train, y_train, x_test, y_test, le\n",
    "  \n",
    "  n_channels = 1\n",
    "\n",
    "  with tf.device('/gpu:0'):\n",
    "    x_train = tf.reshape(x_train, [-1, x_train.shape[1], x_train.shape[2], n_channels])\n",
    "    x_test = tf.reshape(x_test, [-1,  x_test.shape[1], x_test.shape[2], n_channels])\n",
    "\n",
    "\n",
    "  return x_train, y_train, x_test, y_test, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1678788368639,
     "user": {
      "displayName": "DK D",
      "userId": "10687755219747411266"
     },
     "user_tz": -420
    },
    "id": "2pIijWpfoEPQ",
    "outputId": "1aece56e-9804-4e8a-e21c-f7ecbf7318b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['belt' 'vibrato']\n"
     ]
    }
   ],
   "source": [
    "# x_train, y_train, x_test, y_test, le = train_test(train_mfcc, test_mfcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWrqg9lVoER7"
   },
   "source": [
    "# 6) Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbnP7eOR9mVj"
   },
   "source": [
    "### # CNN for Waveform(1d array)\n",
    "+ VocalSet 논문 모델 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfIXtXFf9l95"
   },
   "outputs": [],
   "source": [
    "# (None, 127, 86, 16) \n",
    "def make_waveform_model(input):\n",
    "  model = keras.Sequential()\n",
    "\n",
    "  model.add(layers.Conv1D(input_shape=(44100, 1), filters=16, kernel_size=128, activation='relu'))\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.MaxPooling1D(pool_size=32, strides=8)) # 64\n",
    "  model.add(layers.Dropout(0.3))\n",
    "\n",
    "  model.add(layers.Conv1D(kernel_size=64, filters=8, activation='relu'))\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.MaxPooling1D(pool_size=32, strides=8)) # 64\n",
    "  model.add(layers.Dropout(0.3))\n",
    "\n",
    "  model.add(layers.Conv1D(kernel_size=256, filters=32, activation='relu'))\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.MaxPooling1D(pool_size=32, strides=8)) # 64\n",
    "  model.add(layers.Dropout(0.3))\n",
    "\n",
    "  model.add(layers.Conv1D(kernel_size=16, filters=32, activation='relu'))\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.MaxPooling1D(pool_size=2, strides=2)) # 64\n",
    "  model.add(layers.Dropout(0.3))\n",
    "\n",
    "  # model.add(layers.GlobalAveragePooling1D())\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(units=32, activation='relu', kernel_regularizer=regularizers.L2(1e-3)))\n",
    "  model.add(layers.Dropout(0.3))\n",
    "  model.add(tf.keras.layers.Dense(units=4, activation='softmax', kernel_regularizer=regularizers.L2(1e-3)))\n",
    "\n",
    "  model.summary()\n",
    "  return model\n",
    "\n",
    "tmp = make_waveform_model(1)\n",
    "\n",
    "training_epochs = 1 # 100\n",
    "num_batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "# opt = keras.optimizers.RMSprop(learning_rate=learning_rate) ## SGD, optimizer 바꿔보기\n",
    "\n",
    "audio_model = make_waveform_model(audio_x_train)\n",
    "# keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "# audio_model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
    "audio_model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=learning_rate), metrics=tf.keras.metrics.TopKCategoricalAccuracy(k=2, name='top_k_categorical_accuracy'))\n",
    "early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "audio_model.fit(audio_x_train, audio_y_train, batch_size = num_batch_size, epochs = training_epochs , validation_data = (audio_x_test, audio_y_test) , callbacks = [early])\n",
    "results = audio_model.evaluate(audio_x_test, audio_y_test, batch_size=64)\n",
    "print('test loss, top2 acc :', results)\n",
    "tmp = audio_model.predict(audio_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ryp1elo19lFU"
   },
   "source": [
    "### # CNN for STFT, Mel, MFCC(2d array)\n",
    "+ [Investigating Time-Frequency Representations for Audio Feature Extraction in Singing Technique Classification] 논문 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNC98QFeoEUd"
   },
   "outputs": [],
   "source": [
    "# (None, 127, 86, 16) \n",
    "def make_model(input):\n",
    "  model = keras.Sequential()\n",
    "\n",
    "  model.add(layers.Conv2D(input_shape=(input[0].shape[0], input[0].shape[1], 1), filters=16, kernel_size=(1,2), activation='relu'))\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.MaxPooling2D(pool_size=2))\n",
    "  model.add(layers.Dropout(0.3))\n",
    "\n",
    "  model.add(layers.Conv2D(kernel_size=(1,8), filters=32, activation='relu'))\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.MaxPooling2D(pool_size=2))\n",
    "  model.add(layers.Dropout(0.3))\n",
    "\n",
    "  model.add(layers.Conv2D(kernel_size=(2,1), filters=64, activation='relu')) # 64\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.MaxPooling2D(pool_size=3))\n",
    "  model.add(layers.Dropout(0.3))\n",
    "\n",
    "  model.add(layers.Conv2D(kernel_size=(8,1), filters=128, activation='relu')) # 128\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.MaxPooling2D(pool_size=2)) ##\n",
    "  model.add(layers.Dropout(0.3)) #3\n",
    "\n",
    "  # model.add(layers.GlobalAveragePooling2D())\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(units=64)) # 128\n",
    "  model.add(tf.keras.layers.Dense(units=22))\n",
    "  model.add(tf.keras.layers.Dense(units=4, activation='softmax'))\n",
    "\n",
    "  model.summary()\n",
    "  return model\n",
    "\n",
    "training_epochs = 100\n",
    "num_batch_size = 64\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43493,
     "status": "ok",
     "timestamp": 1678787829589,
     "user": {
      "displayName": "DK D",
      "userId": "10687755219747411266"
     },
     "user_tz": -420
    },
    "id": "9opS2_jfYXym",
    "outputId": "e8880a9b-8dc5-46f0-efdc-a40df6658ac8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 20, 34, 16)        48        \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 20, 34, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 10, 17, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 10, 17, 16)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 3, 17, 128)        16512     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 3, 17, 128)       512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 1, 8, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1, 8, 128)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                65600     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 22)                1430      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 2)                 46        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 84,212\n",
      "Trainable params: 83,924\n",
      "Non-trainable params: 288\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "67/67 [==============================] - 4s 13ms/step - loss: 1.6273 - accuracy: 0.6122 - val_loss: 0.7343 - val_accuracy: 0.6345\n",
      "Epoch 2/100\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 1.1349 - accuracy: 0.6725 - val_loss: 0.7352 - val_accuracy: 0.5845\n",
      "Epoch 3/100\n",
      "67/67 [==============================] - 1s 14ms/step - loss: 0.9351 - accuracy: 0.7033 - val_loss: 0.6363 - val_accuracy: 0.6618\n",
      "Epoch 4/100\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.8264 - accuracy: 0.7209 - val_loss: 0.6022 - val_accuracy: 0.6964\n",
      "Epoch 5/100\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.7574 - accuracy: 0.7310 - val_loss: 0.6003 - val_accuracy: 0.7055\n",
      "Epoch 6/100\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.7049 - accuracy: 0.7369 - val_loss: 0.5596 - val_accuracy: 0.7218\n",
      "Epoch 7/100\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.6213 - accuracy: 0.7533 - val_loss: 0.5595 - val_accuracy: 0.7273\n",
      "Epoch 8/100\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.5786 - accuracy: 0.7670 - val_loss: 0.5308 - val_accuracy: 0.7345\n",
      "Epoch 9/100\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.5471 - accuracy: 0.7712 - val_loss: 0.5276 - val_accuracy: 0.7445\n",
      "Epoch 10/100\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.5199 - accuracy: 0.7752 - val_loss: 0.5343 - val_accuracy: 0.7345\n",
      "Epoch 11/100\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4942 - accuracy: 0.7872 - val_loss: 0.5021 - val_accuracy: 0.7418\n",
      "Epoch 12/100\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4702 - accuracy: 0.7954 - val_loss: 0.5048 - val_accuracy: 0.7418\n",
      "Epoch 13/100\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4598 - accuracy: 0.8046 - val_loss: 0.4973 - val_accuracy: 0.7364\n",
      "Epoch 14/100\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4520 - accuracy: 0.7961 - val_loss: 0.4962 - val_accuracy: 0.7500\n",
      "Epoch 15/100\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.4329 - accuracy: 0.8003 - val_loss: 0.4876 - val_accuracy: 0.7518\n",
      "Epoch 16/100\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4402 - accuracy: 0.7994 - val_loss: 0.4802 - val_accuracy: 0.7473\n",
      "Epoch 17/100\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4308 - accuracy: 0.8046 - val_loss: 0.4741 - val_accuracy: 0.7773\n",
      "Epoch 18/100\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.4229 - accuracy: 0.8076 - val_loss: 0.4600 - val_accuracy: 0.7645\n",
      "Epoch 19/100\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4051 - accuracy: 0.8205 - val_loss: 0.4674 - val_accuracy: 0.7664\n",
      "Epoch 20/100\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.4031 - accuracy: 0.8191 - val_loss: 0.4744 - val_accuracy: 0.7691\n",
      "Epoch 21/100\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4113 - accuracy: 0.8191 - val_loss: 0.4611 - val_accuracy: 0.7691\n",
      "Epoch 22/100\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.4051 - accuracy: 0.8236 - val_loss: 0.4672 - val_accuracy: 0.7736\n",
      "Epoch 23/100\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3970 - accuracy: 0.8262 - val_loss: 0.4730 - val_accuracy: 0.7791\n",
      "Epoch 24/100\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3906 - accuracy: 0.8283 - val_loss: 0.4555 - val_accuracy: 0.7718\n",
      "Epoch 25/100\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3896 - accuracy: 0.8255 - val_loss: 0.4652 - val_accuracy: 0.7855\n",
      "Epoch 26/100\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3843 - accuracy: 0.8306 - val_loss: 0.4722 - val_accuracy: 0.7909\n",
      "Epoch 27/100\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.3854 - accuracy: 0.8288 - val_loss: 0.4711 - val_accuracy: 0.7727\n",
      "Epoch 28/100\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3733 - accuracy: 0.8349 - val_loss: 0.4531 - val_accuracy: 0.7800\n",
      "Epoch 29/100\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3886 - accuracy: 0.8271 - val_loss: 0.4771 - val_accuracy: 0.7864\n",
      "Epoch 30/100\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3786 - accuracy: 0.8285 - val_loss: 0.4552 - val_accuracy: 0.7945\n",
      "Epoch 31/100\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.3838 - accuracy: 0.8299 - val_loss: 0.4878 - val_accuracy: 0.7873\n",
      "Epoch 32/100\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.3788 - accuracy: 0.8278 - val_loss: 0.4810 - val_accuracy: 0.7882\n",
      "Epoch 33/100\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.3651 - accuracy: 0.8393 - val_loss: 0.4741 - val_accuracy: 0.7873\n",
      "Epoch 34/100\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.3689 - accuracy: 0.8370 - val_loss: 0.4771 - val_accuracy: 0.7827\n",
      "Epoch 35/100\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.3704 - accuracy: 0.8370 - val_loss: 0.4883 - val_accuracy: 0.7727\n",
      "Epoch 36/100\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.3624 - accuracy: 0.8452 - val_loss: 0.4715 - val_accuracy: 0.7936\n",
      "Epoch 37/100\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.3661 - accuracy: 0.8374 - val_loss: 0.4548 - val_accuracy: 0.7909\n",
      "Epoch 38/100\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.3618 - accuracy: 0.8405 - val_loss: 0.4733 - val_accuracy: 0.7836\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4733 - accuracy: 0.7836\n",
      "test loss, test acc: [0.47325488924980164, 0.7836363911628723]\n"
     ]
    }
   ],
   "source": [
    "tmp_model = make_model(x_train)\n",
    "tmp_model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
    "early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "tmp_model.fit(x_train, y_train, batch_size = num_batch_size, epochs = training_epochs , validation_data = (x_test, y_test) , callbacks = [early])\n",
    "results = tmp_model.evaluate(x_test, y_test, batch_size=64)\n",
    "print('test loss, test acc:', results)\n",
    "tmp_model.save(\"/content/drive/MyDrive/Startup_Hackathon_Jarvis/model_weight/AWS_test_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FT6w2QwhoEZY"
   },
   "source": [
    "### # CNN for Multi(3d array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LC3mGpMMh683"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xf5i4LQLh7bl"
   },
   "source": [
    "### # resnet + dense(transfer learning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJYfuefwh7i4"
   },
   "outputs": [],
   "source": [
    "# The input must have 3 channels; Received `input_shape=(128, 259, 1)`\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    include_top=False, # 이게 핵심이래\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "    input_shape=(x_train[0].shape[0],x_train[0].shape[1],x_train[0].shape[2]),\n",
    "    pooling='avg',\n",
    ")\n",
    "inputs = Input(shape=(x_train[0].shape[0],x_train[0].shape[1],x_train[0].shape[2]))\n",
    "x = base_model(inputs, training = False)\n",
    "x = Flatten()(x)\n",
    "outputs = Dense(5, activation = 'softmax')(x)\n",
    "model_res = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGEIKYpWh7ng"
   },
   "source": [
    "### # resnet + dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odRxPIRMh7r1"
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.ResNet50(\n",
    "    include_top=True, # 이게 핵심이래\n",
    "    weights=None,\n",
    "    input_tensor=None,\n",
    "    input_shape=(x_train[0].shape[0],x_train[0].shape[1],x_train[0].shape[2]),\n",
    "    pooling='avg',\n",
    ")\n",
    "inputs = Input(shape=(x_train[0].shape[0],x_train[0].shape[1],x_train[0].shape[2]))\n",
    "x = base_model(inputs, training = False)\n",
    "x = Flatten()(x)\n",
    "outputs = Dense(5, activation = 'softmax')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iM0ct0HRh7wD"
   },
   "outputs": [],
   "source": [
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "# model_res.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics = 'accuracy')\n",
    "model_res = tf.keras.Model(inputs, outputs)\n",
    "model_res.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate= 0.001), loss='categorical_crossentropy', metrics = 'accuracy')\n",
    "early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "# train the model on the new data for a few epochs\n",
    "model_res.fit(x_train, y_train, batch_size = 128, epochs = 20, validation_data = (x_test, y_test), callbacks = [early])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldxXQyxwh70H"
   },
   "source": [
    "### # resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vy0plPph74R"
   },
   "outputs": [],
   "source": [
    "res_model = tf.keras.applications.ResNet50(\n",
    "    include_top=True, \n",
    "    weights=None,\n",
    "    input_tensor=None,\n",
    "    input_shape=(x_train[0].shape[0],x_train[0].shape[1],x_train[0].shape[2]),\n",
    "    pooling='avg',\n",
    "    classes = 5\n",
    ")\n",
    "res_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate= 0.001), loss='categorical_crossentropy', metrics = 'accuracy')\n",
    "early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "# train the model on the new data for a few epochs\n",
    "res_model.fit(x_train, y_train, batch_size = 128, epochs = 100, validation_data = (x_test, y_test), callbacks = [early])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9s2A7Inh8Af"
   },
   "source": [
    "### ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3r64cfbBh8Fj"
   },
   "outputs": [],
   "source": [
    "train = read_pickle(name = \"\")\n",
    "test = read_pickle(name = \"\")\n",
    "audio_x_train, audio_y_train, audio_x_test, audio_y_test, le = train_test(train, test, audio_flag = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeCDox0conPY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNr16eL5cUpj6J3s7aX4K9E",
   "collapsed_sections": [
    "xf5i4LQLh7bl",
    "lGEIKYpWh7ng",
    "ldxXQyxwh70H"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
